---
title: "Pvalues.Sets"
author: "Casey Cazer"
Laste updated date: "February 6, 2020"
output: html_document
---

```{r}
#Calculating P values of QM from distribution under null hypothesis of no associations between resistances. The function returns the null distributions of each QM (as the QM value at each percentile). The P values are best calculated outside the function

#requires: expectedQM function
#dbNames: a character vector of labels for the binary databases
#transNames: a character vector of labels for the transaction databases
#setNames: a character vector of labels for the set databases
#rand_dbNames: a character vector of labels for the random (null) databases, must be in same order as transNames
#rand_setNames: a character vector of labels for the sets mined from the random databases, must be in same order as setNames
#minlen: minimum set length to be mined
#maxlen: maximum set length to be mined
#note that minimum support is set at 1 isolate


#function returns a list of: set.csr_pct_avg, set.ecsr_pct_avg, set.lift_pct_avg, set.elift_pct_avg, num_rand_sets

#for future improvement: implement a choice of quality measure to be used or calculate for all quality measures contained in sets@quality. issue warning or statement on average difference between random database item freq and item freq in actual database. allow changing number of rand_db



PValue.sets <- function(dbNames, transNames, setNames, rand_dbNames, rand_setNames, minlen, maxlen){

#for each database of interest, generate 100 random databases with the same item frequencies as the database of interest
for (j in seq_along(rand_dbNames)){
 set.seed(500) #set seed so that function is repeatable
 db <- rep(list(get(dbNames[j])),100) #copy actual data to maintain NA positions
 odb <- list() #storage for optimistic databases without missing data
 n_obs <- nrow(get(transNames[j])) #number of observations
 n_nonNA <- n_obs - colSums(is.na(get(dbNames[j]))) #vector of number of non-NA positions for each antimicrobial
 items <- sapply(get(dbNames[j]), sum, na.rm=TRUE)/n_nonNA #frequency of each item, excluding NA from denominator because binomial generator will also ingore NA positions



  for (i in 1:100){ #for each of the 100 random databases
   for (k in 1:length(items)){ #for each item
     bin_dat <- rbinom(n=n_nonNA[k], size=1, prob=items[k]) #generate random binary data (as 0,1)
     rand_dat <- db[[i]][,k] #get column for AM k
     rand_dat[!is.na(rand_dat)] <- bin_dat #replace non-NA values with random binary data
     db[[i]][,k] <- as.logical(rand_dat) #save
     }
   

   #make optimistic random db by replacing NA with TRUE
    #need to remove AM that were not tested at all since those will be excluded when db is made into transactions. Also need to remove AM that had only FALSE and NA since those will also be exluded from db when it is made into transactions
   odb[[i]] <- db[[i]][colSums(db[[i]], na.rm=TRUE)>0] #keep only AM with at least some TRUE values
   odb[[i]][is.na(odb[[i]])]  <-  TRUE #make optimisitic database where missing data are replaced by resistance
  }
 assign(rand_dbNames[j],list("db"=db, "odb"=odb)) #save to rand_db
}
 
#odb and db must be logical. warn if not. 
if (all(sapply(db, function(x) unlist(sapply(x,is.logical))))==FALSE){
  warning("Random data is not logical.")
}

#next mine sets from rand_db
for (j in seq_along(rand_setNames)){ #for each database
 rand_sets <- vector("list", length=100) #storage for sets
 db <- get(rand_dbNames[j])[["db"]] #get the appropriate random_db and optimistic random_db
 odb <- get(rand_dbNames[j])[["odb"]] 
  
 for (i in 1:100){ #mine each of the 100 rand_db
   #first make transaction sets
   dat <- db[[i]]
   odat <- odb[[i]]
   trans <- as(dat, "transactions") #make transaction database
   otrans <- as(odat, "transactions") #optimistic transaction database
   
   #warn if dimensions are not equal and item frequencies not appropriate--could indicate issue with dropping AM with no resistance from odb
   if(any(dim(trans)!=dim(otrans))){
     warning("Different dimensions in transaction and optmistic_transactions. STOP")
   }
   if(any(itemFrequency(trans)>itemFrequency(otrans))){
     warning("issue with item freq in trans databases")
   }
   
   #next mine sets from trans
   minsup <- 1/length(trans) #min support is default 1 isolate
   sets <- apriori(trans, 
            parameter=list(support=minsup, 
                           maxlen=maxlen, 
                           minlen=minlen, 
                           target="frequent itemsets"),
            control=list(verbose=FALSE)) #mine sets

   #get quality measures
   itemset_list <- LIST(items(sets), decode = FALSE) #need for eSup, eLift, eCSR
   qualmeasures <- interestMeasure(sets, c("crossSupportRatio", "lift"), trans, reuse=TRUE)
   e_qualmeasures <- expectedQM(sets, trans, otrans, dat, itemset_list)

   #store quality measures
   sets@quality$csr <- qualmeasures$crossSupportRatio
   sets@quality$lift <- qualmeasures$lift
   sets@quality$eSup <- e_qualmeasures$eSup
   sets@quality$eLift <- e_qualmeasures$eLift
   sets@quality$eCSR <- e_qualmeasures$eCSR
   sets@quality$cLift <- e_qualmeasures$cLift
   rand_sets[[i]] <- sets #store sets
 }
 assign(rand_setNames[j],rand_sets) #store sets from all 100 random_db
}

  
#summarize number of sets found
num_rand_sets <- data.frame(Data=character(), Min=numeric(), Mean=numeric(), Max=numeric(), real_set=numeric(), stringsAsFactors = FALSE) #create storage

for (j in seq_along(rand_setNames)){ #for each database
 num_rand_sets[j,1] <- transNames[j] #record the label
 num_rand_sets[j,2] <- min(sapply(get(rand_setNames[j]),length)) #minimum number of sets found across random_db
 num_rand_sets[j,3] <- mean(sapply(get(rand_setNames[j]),length)) #average number of sets found across random_db
 num_rand_sets[j,4] <- max(sapply(get(rand_setNames[j]),length)) #maximum number of sets found across random_db
 num_rand_sets[j,5] <- length(get(setNames[j])) #number of sets found in actual db
}


#establish null distribution for each database. The null distribution is defined by the expected (i.e. average) QM at each percentile.
#more resoulution (intervals of <1 percentile) is required at P<=0.05 (95th percentile and up) for Benjamini-Hochberg procedure (although caution should be used in applying BH or BY procedure because actual number of hypotheses tested is unknown). Results in 146 percentile measurements

#storage for the average quality measure (across the 100 random_db) at each percentile; for each db category in transNames
set.lift_pct_avg <- data.frame(
  matrix(ncol=length(transNames), nrow=146, dimnames=list(c(seq(0,0.95,0.01), seq(0.951,1,0.001))))) #one column for each category (in transNames), one row for each percentile

set.csr_pct_avg <- data.frame(
  matrix(ncol=length(transNames), nrow=146,dimnames=list(c(seq(0,0.95,0.01), seq(0.951,1,0.001)))))

set.elift_pct_avg <- data.frame(
  matrix(ncol=length(transNames), nrow=146, dimnames=list(c(seq(0,0.95,0.01), seq(0.951,1,0.001)))))

set.ecsr_pct_avg <- data.frame(
  matrix(ncol=length(transNames), nrow=146,dimnames=list(c(seq(0,0.95,0.01), seq(0.951,1,0.001)))))

set.clift_pct_avg <- data.frame(
  matrix(ncol=length(transNames), nrow=146,dimnames=list(c(seq(0,0.95,0.01), seq(0.951,1,0.001)))))


for (j in seq_along(rand_setNames)){ #for each database category
  
  #storage for the percentiles for each of the 100 random_db
 lift.set <- data.frame(matrix(NA,nrow=146, ncol=100, dimnames=list(c(seq(0,0.95,0.01), seq(0.951,1,0.001))))) #one column for each random_db; one row for each percentile
 
 csr <- data.frame(matrix(NA,nrow=146, ncol=100, dimnames=list(c(seq(0,0.95,0.01), seq(0.951,1,0.001)))))
 
 elift.set <- data.frame(matrix(NA,nrow=146, ncol=100, dimnames=list(c(seq(0,0.95,0.01), seq(0.951,1,0.001)))))
 
 ecsr <- data.frame(matrix(NA,nrow=146, ncol=100, dimnames=list(c(seq(0,0.95,0.01), seq(0.951,1,0.001)))))
 
 clift.set <- data.frame(matrix(NA,nrow=146, ncol=100, dimnames=list(c(seq(0,0.95,0.01), seq(0.951,1,0.001)))))
 
 rand_sets <- get(rand_setNames[j]) #get the random sets associated with this database category

 for (i in 1:100){ #for each of the 100 rand_db
   
  dat <- rand_sets[[i]] #get the sets for that rand_db
  
  lift.set[,i] <- quantile(dat@quality$lift, probs=c(seq(0,0.95,0.01), seq(0.951,1,0.001)), na.rm=TRUE) #calculate lift at each percentile; store
  
  csr[,i] <- quantile(dat@quality$csr, probs=c(seq(0,0.95,0.01), seq(0.951,1,0.001)), na.rm=TRUE) #calculate csr at each percentile
  
  elift.set[,i] <- quantile(dat@quality$eLift, probs=c(seq(0,0.95,0.01), seq(0.951,1,0.001)), na.rm=TRUE) #calculate elift at each percentile
  
  ecsr[,i] <- quantile(dat@quality$eCSR, probs=c(seq(0,0.95,0.01), seq(0.951,1,0.001)), na.rm=TRUE) #calculate ecsr at each percentile
  
  clift.set[,i] <- quantile(dat@quality$cLift, probs=c(seq(0,0.95,0.01), seq(0.951,1,0.001)), na.rm=TRUE) #calculate clift at each percentile
 }

 #calculate average QM value at each percentile across the 100 rand_db (the average across a row). Store values to corresponding column for the db category
 set.lift_pct_avg[,j] <- rowSums(lift.set)/100 #average lift values at each percentile across the 100 rand_db
 set.csr_pct_avg[,j] <- rowSums(csr)/100 #average csr values at each percentile across the 100 rand_db
 set.elift_pct_avg[,j] <- rowSums(elift.set)/100 #average elift values at each percentile across the 100 rand_db
 set.ecsr_pct_avg[,j] <- rowSums(ecsr)/100 #average ecsr values at each percentile across the 100 rand_db
 set.clift_pct_avg[,j] <- rowSums(clift.set)/100 #average elift values at each percentile across the 100 rand_db
 
 #assign category name
 colnames(set.lift_pct_avg)[j] <- transNames[j]
 colnames(set.csr_pct_avg)[j] <- transNames[j]
 colnames(set.elift_pct_avg)[j] <- transNames[j]
 colnames(set.ecsr_pct_avg)[j] <- transNames[j]
 colnames(set.clift_pct_avg)[j] <- transNames[j]
}


#put all output in a list
out <-list(set.csr_pct_avg, set.ecsr_pct_avg, set.lift_pct_avg, set.elift_pct_avg, set.clift_pct_avg, num_rand_sets) #output to return
names(out) <- c( "set.csr_pct_avg", "set.ecsr_pct_avg", "set.lift_pct_avg", "set.elift_pct_avg", "set.clift_pct_avg", "num_rand_sets")

out
}
```


####Instructions for P-value calculation for actual sets based on the null distributions

#the P-value calculation is best done outside the function in order to append p-values directly to class itemsets
```{r, eval=FALSE}
#P-value is the percent of sets in the random data that would be expected to have a QM >= the QM of the actual set

#First finds the position (percentile) of each actual QM value in the relevant null distribution. 
##Detect_index searches from 100th percentile down, returning the row.number of the null distribution that is the first with a QM value less than the actual QM value. 
###The true percentile is slightly greater than the returned value (lies between the returned value and the next greatest percentile; unless the actual QM is = the null percentile QM). Since the percentiles are unequally spaced (greater resolution at 0.95 to 1), this code uses the row name to report the percentile

#must recode 0 as 1 (mapvalues) to correctly reference row.names. position=0 indicates that the QM was smaller than the smallest QM in null distribution
#P-value is 1 - percentile. It is an upper bound; the true P-value is slightly less than reported since the true percentile is slightly greater than reported

for (i in seq_along(setNames)){ #for each group of sets
  sets <- get(setNames[i]) #get the sets

  #find percentile of each actual QM, which is reported as the row.name from the detect_index and then made numeric
  perc.ecsr <- as.numeric(row.names(set.ecsr_pct_avg)[adply(sets@quality$eCSR, .margins=1, function(x) detect_index(set.ecsr_pct_avg[,i], function(y) y<=x, .dir="backward"), .id=NULL)[,1] %>% mapvalues(0,1, warn_missing=FALSE)])

  perc.csr <- as.numeric(row.names(set.csr_pct_avg)[adply(sets@quality$CrossSupRatio, .margins=1, function(x) detect_index(set.csr_pct_avg[,i], function(y) y<=x, .dir="backward"), .id=NULL)[,1] %>% mapvalues(0,1, warn_missing=FALSE)])

   perc.eLift <- as.numeric(row.names(set.elift_pct_avg)[adply(sets@quality$eLift, .margins=1, function(x) detect_index(set.elift_pct_avg[,i], function(y) y<=x, .dir="backward"), .id=NULL)[,1] %>% mapvalues(0,1, warn_missing=FALSE)])

    perc.lift <- as.numeric(row.names(set.lift_pct_avg)[adply(sets@quality$lift, .margins=1, function(x) detect_index(set.lift_pct_avg[,i], function(y) y<=x, .dir="backward"), .id=NULL)[,1] %>% mapvalues(0,1, warn_missing=FALSE)])

    perc.cLift <- as.numeric(row.names(set.clift_pct_avg)[adply(sets@quality$cLift, .margins=1, function(x) detect_index(set.clift_pct_avg[,i], function(y) y<=x, .dir="backward"), .id=NULL)[,1] %>% mapvalues(0,1, warn_missing=FALSE)])

  #save as P-values
  sets@quality$Pval.eCSR <- 1-perc.ecsr
  sets@quality$Pval.CSR <- 1-perc.csr
  sets@quality$Pval.eLift <- 1-perc.eLift
  sets@quality$Pval.lift <- 1-perc.lift
  sets@quality$Pval.cLift <- 1-perc.cLift

  assign(setNames[i], sets)
  rm(sets, perc.ecsr, perc.csr, perc.eLift, perc.lift, perc.cLift)
}
```
