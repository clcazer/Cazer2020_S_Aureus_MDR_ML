---
title: "Rule Mining Functions"
author: "Casey Cazer"
date: Copied 11/18/2019 from Association Mining Functions Git. Last updated 2/6/2020
---

When data is missing in the transaction database, using expected support is more appropriate than the support in aRules.

measuring expected support (Kryszkiewicz 1999: Incomplete database issues for representative association rules)
$$eSup(X) = \frac{pSup(X)}{pSup(X)+pSup(-X)}$$
$pSup(X)= \frac{n(X)}{D}$, the number of tranactions that include X divided by total observations (the support currently implemented in aRules)
-X is the 'anti-pattern'. It can have missing values for items as long as the values for some of the items do not match the values in X (e.g. some of the AM that are R in X are S in -X) and so the pattern cannot be X regardless of the missing values' true values.

to calculated eSup, we need to calcue pSup(-X).

if missing values were populated with the values in X (e.g. if all missing values were replaced with R), then $1 = sup(+X) + sup(-X)$ and $1 - sup(+X) = sup(-X)$, where +X is the pattern with the same R items as X, including transactions with missing values that were populated with R. $sup(+X)=oSup(X)$ -> optimistic support of X. Therefore, $pSup(X) + pSup(-X) = pSup(X) + 1 - oSup(X) = 1 - (oSup(X) - pSup(X)) = 1 - dSup(X)$ (see below)

(Kryszkiewicz 1999): $eSup(X) = \frac{pSup(X)}{1-dSup(X)}$, where $dSup(X)= oSup(X) - pSup(X)$ and $oSup(X)=\frac{m(X)}{D}$, where m(X) is the number of observtions that may match the pattern X.

Hence: $$eSup(X) = \frac{pSup(X)}{1 - (oSup(X) - pSup(X))}$$

So in order to calculate expected support, we just need to calculate optimistic support, which is easy by just replacing missing values in the database by the values in the pattern X (i.e. replacing missing values with Resistant)

#####
Abbreviations/Definitions:
'itemset' (used interchangeably with 'set' or 'pattern') consists of individual 'items' (in this case, 'AM')
AM: Antimicrobial
trans: transactions (see arules)
set database: outcome of apriori mining step (see arules)
QM: quality measure

```{r}
#eSupport function calculates expected Support for each itemset in a set database (currently implemented for one set database at a time so that eSupport can be implemented within apriori mining step)
#requires:
#transName: a character label for the transaction database used to mine set database
#setName: a character label for the set database
#oTransName: a character label for the optimistic transaction database with missing values replaced by TRUE. must have same columns as transaction database
#setList: itemset_list <- LIST(items(setName), decode = FALSE). Faster to create this variable outside the function since it is used in eSupport, eCSR, and eLift
expectedSupport <- function(setName, transName, oTransName){
  if(any(dim(trans)!=dim(otrans))){ #transaction and optimistic transaction databases must have same columns
    warning("Different dimensions of transactions and optimistic transactions. STOP")
  }
  
  pSup <- interestMeasure(setName, "support", transName, reuse=TRUE) #pSup is standard support calculated by arules
  oSup <- interestMeasure(setName, "support", oTransName, reuse=FALSE) #oSup is the standard support calculated when missing values are set to TRUE/resistant
  eSup <- pSup/(1- (oSup - pSup)) #expected support
  
  if (all((pSup - eSup) < 1e-10)==FALSE){ #pSup should be <= eSup although a rounding error due to division in calculating eSup may result in small positive value
    warning("Expected Support is less than Pessimistic Support.") #if pSup>eSup, there may be something wrong with optimistic transaction database
  }
  
  if (all((eSup - oSup) < 1e-10)==FALSE){ #eSup should be <= oSup although a rounding error due to division in calculating eSup may result in small positive value
    warning("Optimistic Support is less than Expected Support.") #if eSup>oSup, there may be something wrong with optimistic transaction database
  }

  eSup
}


#expectedLift calculates expected lift in the same way as lift (implemented in arules) but using expected support
#requires:
#transName: a character label for the transaction database used to mine set database
#oTransName: a character label for the optimistic transaction database with missing values replaced by TRUE
#setList: itemset_list <- LIST(items(setName), decode = FALSE). Faster to create this variable outside the function since it is used in eSupport, eCSR, and eLift
#eSup: expected support vector for itemsets in the setList, generated by expectedSupport function
expectedLift <- function (transName, oTransName, setList, eSup){
  pItemSupport <- itemFrequency(transName) #pSup for each individual AM
  oItemSupport <- itemFrequency(oTransName) #oSup for each individual AM
  eItemSupport <- pItemSupport/(1-(oItemSupport-pItemSupport)) #eSup for each individual AM
  
  eLift <- eSup / sapply(setList, function(i) prod(eItemSupport[i])) #eLift = eSup(itemset) / product of eSup for individual AM in the itemset
  
  eLift
}


#expectedCSR calculates expected CSR (cross support ratio) in the same was as crossSupportRatio (implemented in arules) but using expected support
#requires:
#transName: a character label for the transaction database used to mine set database
#oTransName: a character label for the optimistic transaction database with missing values replaced by TRUE
#setList: itemset_list <- LIST(items(setName), decode = FALSE). Faster to create this variable outside the function since it is used in eSupport, eCSR, and eLift
expectedCSR <- function(transName, oTransName, setList){
  pItemSupport <- itemFrequency(transName) #pSup for each individual AM
  oItemSupport <- itemFrequency(oTransName) #oSup for each individual AM
  eItemSupport <- pItemSupport/(1-(oItemSupport-pItemSupport)) #eSup for each individual AM
  
  eCSR <- sapply(setList, function(i) min(eItemSupport[i])) / sapply(setList, function(i) max(eItemSupport[i])) #eCSR = min eSup(individual AM set) / max eSup (individual AM in set)
  
  eCSR
}


#conditional lift: uses conditionalSupport, which is the support conditional on all AMs in the set being tested (e.g. no NA). This accounts for situations where testing of AM is nearly orthogonal--e.g. typically only one of the is tested and the others are not. This can result in itemset lift values <1 when subsets of the itemset may have lift>1 because the support of the whole itemset is low due to few isolates being tested against both AM.
#numerator is the support count of the entire pattern divided by the number of isolates with all the AM in the pattern tested (i.e. conditional Support)
#denominator is the product of the conditional support for each AM in the pattern
#requires:
#transName: a character label for the transaction database used to mine set database
#dbName: a character label for the database (containing T/F/NA for each AM and isolate) used to create the transaction database. 
#setList: itemset_list <- LIST(items(setName), decode = FALSE). Faster to create this variable outside the function since it is used in eSupport, eCSR, and eLift
conditionalLift <- function(transName, setList, dbName){
  AMsInSet <- lapply(setList, function(i) itemLabels(transName)[i]) #for each itemset, a character vector of the AMs in the set
  OnlyAMsInSet <- lapply(AMsInSet, function(i) subset(dbName, select=i) %>% na.omit()) #in the binary database, select only AM columns in the set and isolates that had been tested against all of them (no NA). creates a list of binary databases, one for each itemset
  setCount <- interestMeasure(setName, "support", transName, reuse=TRUE) * length(transName) #for each itemset, the number of isolates with the set (all AM in the set must have been tested in these isolates for them to be counted in the support)
  numerator <- setCount/ldply(OnlyAMsInSet, nrow) #P(set | all AM in set are tested)
  denominator <- ldply(OnlyAMsInSet, function(i) prod(sapply(i, mean))) #find the support of each individual AM in the set (mean applied to logical vector) and multiply them together
  cLift <- numerator/denominator
}

#put all together in one function, saves computation time if you need more than one of QM derivatives
expectedQM <- function(setName, transName, oTransName, dbName, setList){
  if(any(dim(transName)!=dim(oTransName))){ #transaction and optimistic transaction databases must have same columns
    warning("Different dimensions of transactions and optimistic transactions. STOP")
  }
  
  if(length(setName)!=length(setList)){ #setList must correspond to the sets
    warning("Different number of sets in setName and setList. STOP")
  }
  
  #calculate pSup, oSup, eSup for each itemset
  pSup <- interestMeasure(setName, "support", transName, reuse=TRUE)
  oSup <- interestMeasure(setName, "support", oTransName, reuse=FALSE)
  eSup <- pSup/(1- (oSup - pSup))
  
  #check that expected conditions are met
  if (all((pSup - eSup) < 1e-10)==FALSE){ #pSup should be <= eSup although a rounding error due to division in calculating eSup may result in small positive value
    warning("Expected Support is less than Pessimistic Support.")
  }
  
  if (all((eSup - oSup) < 1e-10)==FALSE){ #eSup should be <= oSup although a rounding error due to division in calculating eSup may result in small positive value
    warning("Optimistic Support is less than Expected Support.")
  }

  #calculate pSup, oSup, eSup for each individual AM
  pItemSupport <- itemFrequency(transName)
  oItemSupport <- itemFrequency(oTransName)
  eItemSupport <- pItemSupport/(1-(oItemSupport-pItemSupport))
  
  #eLift
  eLift <- eSup / sapply(setList, function(i) prod(eItemSupport[i]))
  
  #eCSR
  eCSR <- sapply(setList, function(i) min(eItemSupport[i])) / sapply(setList, function(i) max(eItemSupport[i]))
  
  #cLift
  AMsInSet <- lapply(setList, function(i) itemLabels(transName)[i]) #for each itemset, a character vector of the AMs in the set
  OnlyAMsInSet <- lapply(AMsInSet, function(i) subset(dbName, select=i) %>% na.omit()) #in the binary database, select only AM columns in the set and isolates that had been tested against all of them (no NA). creates a list of binary databases, one for each itemset
  setCount <- interestMeasure(setName, "support", transName, reuse=TRUE) * length(transName) #for each itemset, the number of isolates with the set (all AM in the set must have been tested in these isolates for them to be counted in the support)
  numerator <- setCount/ldply(OnlyAMsInSet, nrow) #P(set | all AM in set are tested)
  denominator <- ldply(OnlyAMsInSet, function(i) prod(sapply(i, mean))) #find the support of each individual AM in the set (mean applied to logical vector) and multiply them together
  cLift <- numerator/denominator
  
  #save together as dataframe
  eQM <- as.data.frame(cbind(pSup, oSup, eSup, eLift, eCSR, cLift))
  colnames(eQM) <- c("pSup", "oSup", "eSup", "eLift", "eCSR", "cLift")
  eQM
}
```


turn sets into edges and nodes for plotting
```{r}
#SetDataFrame: use itemset_details dataframe (output of QM_for_all_itemsets or closed_sets or all_sets).  must have column of itemsets (items divided by commas), column of category/group, column(s) of quality measure(s) you want to use in plots (must be numeric)
##SetColumn: character string for the itemset column name in SetDataFrame
##CatColumn: character string for the category/grouping column name in SetDataFrame
##index: column numbers (index) in SetDataFrame for quality measures that you want to be averaged for each edge (by category/group)
##QMNames: character vector of the QM names in index
#AM_class: dataframe connecting each item (AM) to an AM class with column names "AM" and "Code"

#resulting edges are weighted by number of sets contributing to each edge

plot_sets_weighted<-function(SetDataFrame, SetColumn, CatColumn, index, QMNames, AM_class){
  #required packages
  require(dplyr)
  require(tidyr)
  require(stringr)
  require(splitstackshape)
  
  sets <- dplyr::rename(SetDataFrame, items=SetColumn, cat=CatColumn) #from SetDataFrame, rename SetColumn and CatColumn for consistent use later
  
  if (any(sapply(sets[,index], is.numeric)==FALSE)){ #QM must be numeric
    stop("Some quality measures are not numeric.")
  }
  
  #give each itemset an ID (set#) and Name (category: items)
  sets$setID <- paste("set", seq(1, nrow(sets),1), sep="")
  sets$setName <- paste(as.character(sets$cat), sets$items, sep=":")
  
  
  edges <- as.data.frame(NULL) #dataframe to store edges
  
  for (i in 1:nrow(sets)){ #for each itemset
    items <- str_split(sets$items[i], ",")[[1]] #take character string of items divided by "," and turn into character vector

    #generate all possible 2-item combinations from the itemset
    if (length(items)>2){ #if there are more than two items
      combos <- as.data.frame(t(combn(items, 2))) #puts 2-item combos in a dataframe, with each item in a column, in alphabetical order within the row
      dat <- combos #prepare to add more info to dataframe
      colnames(dat) <- c("Node1", "Node2") #name each item as a Node
      otherdat <- expandRows(sets[i,], nrow(combos), count.is.col = FALSE) #copy the QM info associated with the itemset to each 2-AM combination
      dat <- cbind(dat, otherdat) #add QM data to the Nodes
      
    }else{ #2-item itemsets (no combinations to make)
      dat <- cbind(Node1=items[1], Node2=items[2], sets[i,]) #if no combos to make, just split the items into Node1 and Node 2, copy the QM info
    }
    
    edges <- rbind(edges, dat) #bind all itemset edges together
  }
  
  edges$edgeName <- paste(edges$Node1, edges$Node2) #create edge name from the node names.
  
  with(edges,aggregate(formula(paste('cbind(',paste(QMNames, collapse=","), ')~edgeName+cat', sep="")), FUN=function(x) mean(x, na.rm = TRUE), na.action=na.omit)) -> edges_aggregated #average QM for each edge, grouped by category

 separate(edges_aggregated, edgeName, into=c("Node1", "Node2"), sep=" ", remove=FALSE) -> edges_aggregated #separate the edgeName into Node1 and Node2 (these were dropped from 'edges' in the previous step)

 #identify AM class of each node
  edges_aggregated$Node1Class <- AM_class[match(edges_aggregated$Node1,AM_class$AM),]$Code
  edges_aggregated$Node2Class <- AM_class[match(edges_aggregated$Node2,AM_class$AM),]$Code

  
  #edge weight is the number of rules that contain the edge
  weight <- dplyr::count(edges, edgeName, cat) #within edges, count the number of times that each edgeName appears in each category
  edges_aggregated <- merge(edges_aggregated, weight, by=c("edgeName", "cat")) #append weight to edges_aggregated

  #edge id is numerical sequence: e1, e2...
  edges$ID=paste("e",seq(1,nrow(edges),1),sep="")
  
  edges_aggregated
}
```


combine sets found across several datasets (all.sets) into one dataframe
```{r}
#best_setNames: vector of the sets you want to combine. 
#labelNames: vector of dataset names (labels) that will be used to identify the origin of each set

#the function all_sets collates itemsets across several datasets and outputs a list containing (1) dataframe of all itemsets and associted QM (nrow = sum of number of sets in each category), dataframes of itemset quality across all datasets for (2) support, (3) CSR, (4) lift, (5) expected support, (6) expected CSR, (7) expected lift, (8) conditional lift. For dataframes 2:8, nrow = number of unique itemsets across all categories; ncol = number of categories + 'items' + 'order'. The quality measures are only included in each category (dataset) if the itemset is found in the itemset datasets provided. If filtering criteria have been applied, no quality measure info will be provided if the set exists in the original datset but does not meet the filtering criteria.

all_sets <- function (best_setNames, labelNames){
  #required packages
  require(tidyr)
  require(stringr)
  
  #first, sets must be transformed from class itemsets to data frame. uses setsAsDataFrame function
  setsAsDataFrame <- function(sets, cat) {
    itemsets <- labels(items(sets)) #itemset names
  
    #create dataframe with category and relevant quality measures
    data.frame(
      Category <- rep(cat, length(sets)),
      items <- itemsets,
      support <- quality(sets)$support,
      count <- quality(sets)$count,
      csr <- quality(sets)$CrossSupRatio,
      lift<-quality(sets)$lift,
      eSup <- quality(sets)$eSup,
      eCSR <- quality(sets)$eCSR,
      eLift <- quality(sets)$eLift,
      cLift <- quality(sets)$cLift
   )
  }
  
  
  #for storing itemsets
  all.sets <- data.frame()
  
  #turn each itemset database into dataframe and label with dataset name, then combine with other itemset dataframes
  for (i in seq_along(best_setNames)){ 
    dat <- get(best_setNames[i])
    cat <- labelNames[i] #labels for each dataset
    all.sets <- rbind(all.sets, setsAsDataFrame(dat, cat))
  }
  #appropriate column names
  colnames(all.sets) <- c("Category", "items", "support", "count", "csr", "lift", "eSup", "eCSR", "eLift", "cLift")

  #calculate the itemset order (number of AM in the set)
  all.sets$order <- str_count(all.sets$items, ",")+1 #in the itemset string, AM are divided by ",": count the commas and add 1 for the order

  #tabulate quality measures for each itemset in each database; display with each unique itemset as a row and the QM value in each category as the columns
  all.sets.sup <- select(all.sets, "Category", "items", "support", "order") #support in each itemset. drop other QM columns
  all.sets.sup <- spread(all.sets.sup, Category, support, drop=TRUE) #create one column for each category, place support of itemset, within category, in those columns. reduces to one row per itemset
  
  #repeat for other QM
  
  all.sets.csr <- select(all.sets, "Category", "items", "csr", "order") #csr in each itemset
  all.sets.csr <- spread(all.sets.csr, Category, csr, drop=TRUE)

  all.sets.lift <- select(all.sets, "Category", "items", "lift", "order") #lift in each itemset
  all.sets.lift <- spread(all.sets.lift, Category, lift, drop=TRUE)
  
  
  #expected measures
  all.sets.eSup <- select(all.sets, "Category", "items", "eSup", "order") #expected support in each itemset
  all.sets.eSup <- spread(all.sets.eSup, Category, eSup, drop=TRUE)
  
  all.sets.eCSR <- select(all.sets, "Category", "items", "eCSR", "order") #csr in each itemset
  all.sets.eCSR <- spread(all.sets.eCSR, Category, eCSR, drop=TRUE)

  all.sets.eLift <- select(all.sets, "Category", "items", "eLift", "order") #elift in each itemset
  all.sets.eLift <- spread(all.sets.eLift, Category, eLift, drop=TRUE)
  
  all.sets.cLift <- select(all.sets, "Category", "items", "cLift", "order") #clift in each itemset
  all.sets.cLift <- spread(all.sets.cLift, Category, cLift, drop=TRUE)
  
  #save all dataframes in a list
  all.sets.out <- list(all.sets, all.sets.sup, all.sets.csr, all.sets.lift, all.sets.eSup, all.sets.eCSR, all.sets.eLift, all.sets.cLift) #put all results into a list
  
  names(all.sets.out) <- c("all.sets", "all.sets.sup", "all.sets.csr", "all.sets.lift", "all.sets.eSup", "all.sets.eCSR", "all.sets.eLift", "all.sets.cLift")
  
  all.sets.out #return the list of dataframes
}
```


itemset to class-set
take an itemset {A, B, C}, use a classification table (e.g. A-class1; B-class1; C-class2) to turn the item-set into a class-set in alphanumeric order {1,2}
```{r}
#requires an itemset dataframe with a column containing the items in form {A,B,C} ("items") and a column containing the itemset order ("order", number of items)
#intended to be applied to dataframes resulting from all_sets function
#AM_class: a classification table with AM names ("AM") and classes ("Class") and class-codes ("Code"). 'classes' should be one letter or number but 'class-codes' can be a multi-letter code for the class.

itemset_to_class_set <- function(data, AM_class){
  #required packages
  require(stringr)
  
  #storage for class info
  unique_class <- rep("NA", nrow(data)) 
  num_class <- rep("NA", nrow(data))
  unique_code <- rep("NA", nrow(data))
  
  #first take each itemset (character string) and turn into character vector, with each item as separate strings
  itemset <- sapply(data$items, function(x) str_replace_all(x, c("\\{|\\}"),"")) #removes { and } from itemset character string
  items <- sapply(itemset, function(x) str_split(x, ",")[[1]]) #returns a list with each element being a character vector containing the items in an itemset

  for (i in 1:length(items)){ #for each itemset
    index <- match(items[[i]], AM_class$AM) #match the items in the set to AM_class
    class <- AM_class$Class[index] #get the AM Classes for each item
    
    #replace Class with more informative Code, if available
    if (is.null(AM_class$Code)==FALSE) { #if Code is available
    code <- AM_class$Code[index] #get the AM Class Codes for each item
    unique_code[i] <- paste(as.character(sort(unique(code))), collapse=",") #paste together alphabetically
    }
    #otherwise, if more informative Code is not available
    unique_class[i] <- paste(as.character(sort(unique(class))), collapse=",") #paste unique classes alphanumerically
    num_class[i] <- length(unique(class)) #number of classes in the itemset
  }
    data$Classes <- unique_class #append to data
    data$NumClasses <- num_class #append to data
    
    if (is.null(AM_class$Code)==FALSE) { #if Code is available, append to data
    data$ClassCodes <- unique_code
    }
    
    data
}
```


Bootstrap intervals for quality measures
```{r}
#QM_bootstrap_CI calculates the bootstrap distribution for a given quality measure. Returns a list with with one element per itemset. Each list element is a dataframe of the QM values for all the bootstrap resamples
#requires the function expectedQM\
#dbName: character name of the binary dataset (dataframe)
#transName: character name of the transaction dataset
#setList: itemset_list <- LIST(items(setName)) #if decode=FALSE is added, then items are listed by integer rather than name
#times: number of bootstrap replicates
#QM: character name of the quality measure to calculate bootstrap percentiles. support and count are automatically included. must match apiori interestMeasure options. e.g. "lift"
QM_bootstrap_CI <- function (dbName, transName, setList, times){
  require(plyr)
  require(dplyr)
  require(purrr)
   
  db <- get(dbName)
  trans <- get(transName)
  
  #generate bootstrap transaction datasets of size n-1
  set.seed(500)
  
  #first as a dataframe, then turn into transactions
  bootstrap_db_samples <- rlply(times, sample_n(db, size=(nrow(db)-1), replace=TRUE))
  bootstrap_samples <- lapply(bootstrap_db_samples, function(x) as(x,"transactions")) 
  
  #mine sets from each bootstrap transaction datasets. same parameters as mining in analysis
  minsup <- 1/length(trans)
  minlen <- 1
  maxlen <- dim(trans)[2] #maxlen is using all items
  
  bootstrap_samples_mined <- lapply(bootstrap_samples, 
                             function(x) {
                             apriori(x, parameter=list(support=minsup, target="frequent itemsets", minlen=minlen, maxlen=maxlen), control=list(verbose=F))
                               }
                             ) #gives only support, count as QM. Need to apply expectedQM

#oTrans required for expectedQM calculation
  #select only AM columns from the bootstrap_db that are in the actual transaction database (otherwise columns with all NA will be included)
  bootstrap_odb <- mapply(function(db, trans) select(db, itemLabels(trans)), bootstrap_db_samples, bootstrap_samples) #from each db (list element) in bootstrap_db_samples, select columns that match the itemLabels from the corresponding transaction database (list element) in bootstrap_samples
  
  #replace NA with TRUE (only in columns present in the transaction db) to create optimistic transaction database
  bootstrap_otrans <- mapply(function(odb,trans) {
    rep.NA <- as.list(rep(as.logical("TRUE"), length(itemLabels(trans)))) #generate list (one element for each AM column) of "TRUE" to use in replace_na
    names(rep.NA) <- itemLabels(trans) #replace_na requires named (col names) list
    replace_na(odb, rep.NA) #replace NA with True in odb
    as(odb, "transactions") #make transactions
    },
    bootstrap_odb, bootstrap_samples) #for each optimistic db in bootstrap_odb, replace NA values with "TRUE", using the transaction database (bootstrap_samples) to specify the columns that should be modified. If a binary (non-transaction) db is used to specify columns instead, then all columns with no tested isolates (e.g. column is all NA) will be modified to all TRUE and inappropriately included in optimistic transactions
 
  
  #calculate QM for all sets in each bootstrap transaction dataset
  bootstrap_sets_QM <- mapply(function(sets, trans, db, otrans) {
  
    boot.set_list <- LIST(items(sets), decode = FALSE)
    sets@quality$CrossSupRatio <- interestMeasure(sets, "crossSupportRatio", trans, reuse=TRUE)
    sets@quality$lift <- interestMeasure(sets, "lift", trans, reuse=TRUE)
    eQM <- expectedQM(sets, trans, otrans, db, boot.set_list)
    sets@quality$eSup <- eQM$eSup
    sets@quality$eCSR <- eQM$eCSR
    sets@quality$eLift <- eQM$eLift
    sets@quality$cLift <- eQM$cLift
    sets},
    bootstrap_samples_mined, bootstrap_samples, bootstrap_db_samples, bootstrap_otrans)
  
   
#get the bootstrap values of the QM for each set. save as list with each set as a list element
bootstrapQM <- lapply(setList, #for each set in setList
                        function(y) 
                        {ldply(bootstrap_sets_QM, #apply to each bootstrap sample
                              possibly(function(x) slot(subset(x, 
                                                subset = items %ain% y &  
                                                items %oin% y), "quality"), data.frame("support"=0, "count"=0, "CrossSupRatio"=0, "lift"=0, "eSup"=0, "eCSR"=0, "eLift"=0, "cLift"=0)))}) #return the QM for the set in each bootstrap sample (from bootstrap_sets_QM). if an item in set was not tested/included in sample, give 0's
  
  #apply set names as the names of list elements
  names(bootstrapQM) <- unlist(lapply(setList, function(x) paste(x, collapse=","))) 
  bootstrapQM #output
}

#outside of function, use this to get percentile intervals for each set for selected QM

  #bootstrapCI <- ldply(bootstrapQM, function(x) quantile(pull(x,QM), c(0.025,0.975), names=FALSE)) 
  
  #names(bootstrapCI) <- c("items", paste(QM, "Boot0.025", sep=""), paste(QM,"Boot0.975", sep=""))
  
```